# Paper 02 Results: Liu et al. (2022) - Effective Theory

**Date**: November 4, 2025  
**Status**: â­ **GROKKING CONFIRMED!** â­

---

## ğŸ¯ Key Results

### Grokking Detection
- âœ… **Train accuracy threshold (90%)** reached at: **step 1130**
- âœ… **Test accuracy threshold (90%)** reached at: **step 1530**
- â­ **GROKKING DELAY: 400 steps** - Test lagged behind train!
- âœ… **RQI threshold (95%)** reached at: **step 800**

### Final Performance
- **Train Accuracy**: 100.0%
- **Test Accuracy**: 100.0%
- **RQI (Representation Quality Index)**: 100.0%

### Training Duration
- **Total steps**: 5,000
- **Runtime**: 21 seconds on A100 GPU
- **Configuration**: p=10, 45/55 train/test split

---

## ğŸ“Š What Makes This Paper Special

### Novel Contribution: Representation Quality Index (RQI)

This paper introduced **RQI** as a quantitative measure of representation quality:
- RQI = 1.0 means perfect structured representation
- RQI < 1.0 means noisy/unstructured representation

**Finding**: RQI improved FIRST (step 800), then train accuracy (step 1130), then test accuracy (step 1530)

This shows: **Good representations â†’ Memorization â†’ Generalization**

### The Four Phases of Learning

The paper categorizes learning into 4 phases:

1. **Comprehension**: Good train + good test (ideal!)
2. **Grokking**: Good train + delayed test improvement
3. **Memorization**: Good train + poor test (overfitting)
4. **Confusion**: Poor train + poor test (not learning)

**Our result**: Shows the **Grokking phase** clearly!

---

## ğŸ“ˆ Training Trajectory

### Early Phase (Steps 0-1000)
- Train acc: 4.4% â†’ ~60%
- Test acc: 20% â†’ ~50%
- RQI: Low â†’ ~0.8
- Status: Initial learning

### Grokking Phase (Steps 1000-2000)
- **Step 1130**: Train accuracy hits 90% threshold âœ…
- **Step 1530**: Test accuracy hits 90% threshold âœ… (400 steps later!)
- RQI: Already at ~0.95
- Status: **Delayed generalization occurring**

### Post-Grokking (Steps 2000-5000)
- Train acc: 90% â†’ 100%
- Test acc: 90% â†’ 100%
- RQI: 0.95 â†’ 1.0
- Status: Refinement and convergence

---

## ğŸ”‘ Key Hyperparameters

### Model Architecture
```python
encoder_width = 200
encoder_depth = 3
decoder_width = 200
decoder_depth = 3
hidden_rep_dim = 1        # 1D internal representation
activation = tanh
```

### Training Configuration
```python
p = 10                    # 10 symbols (mod 10)
train_num = 45            # 45/55 train/test split (81.8% train)
steps = 5000              # Only 5000 steps needed!
eta_reprs = 1e-3          # LR for representations
eta_dec = 1e-4            # LR for decoder (10x smaller!)
weight_decay_reprs = 0.0  # NO weight decay
weight_decay_dec = 0.0    # NO weight decay
seed = 58                 # Paper's seed
```

### Why This Works
1. **Different learning rates**: Representations learn 10x faster than decoder
2. **No weight decay needed**: Unlike other papers (which use WD=1.0)
3. **Small dataset**: Only 45 training samples forces generalization
4. **Structured task**: Modular addition has inherent structure (parallelograms)

---

## ğŸ“ Generated Files

### Data
- `02_liu_et_al_2022_effective_theory/logs/training_history.json` (9.4 KB)
  - All 5,000 training steps logged
  - Includes train/test acc, loss, and RQI

### Visualizations
- `analysis_results/paper_02_grokking.png` (592 KB)
  - 4-panel figure:
    1. **Accuracy curves** with grokking delay annotation
    2. **Loss curves** (log scale)
    3. **RQI trajectory** (unique to this paper!)
    4. **Generalization gap** visualization

- `analysis_results/paper_02_grokking_zoomed.png` (183 KB)
  - Zoomed view of grokking transition region
  - Shows steps 630-2030 (around the transition)
  - Highlights the 400-step delay clearly

---

## ğŸ”¬ Scientific Insights

### What We Confirmed
1. âœ… **Delayed generalization exists** - test accuracy improved 400 steps after train
2. âœ… **Representations matter** - RQI improved before both accuracies
3. âœ… **Structure emerges** - Model discovered parallelogram structure
4. âœ… **Perfect final performance** - 100% train and test (not just overfitting!)

### How This Differs from Other Grokking Papers

| Aspect | Paper 01 (Power) | Paper 02 (Liu) | Paper 03 (Nanda) |
|--------|------------------|----------------|------------------|
| Architecture | Transformer | Encoder-Decoder MLP | Transformer |
| Steps needed | ~100k | 5k | 40k |
| Weight decay | 1.0 (critical) | 0.0 (not needed!) | 1.0 |
| Novel metric | - | **RQI** | Progress measures |
| Grokking style | Sharp jump | Smooth transition | Multiple jumps |

**Key finding**: Grokking can occur **without weight decay** if you have the right architecture!

---

## ğŸ¨ Figure Descriptions

### Main Figure (paper_02_grokking.png)

**Top Left - Accuracy Over Time**:
- Blue line: Train accuracy
- Red line: Test accuracy
- Blue dashed: Train reaches 90% (step 1130)
- Red dashed: Test reaches 90% (step 1530)
- Green arrow: Shows 400-step grokking delay

**Top Right - Loss Curves**:
- Exponential decay in both train and test loss
- Losses converge to near-zero
- Log scale shows full dynamic range

**Bottom Left - RQI Trajectory**:
- Purple line: Representation Quality Index
- RQI improves first (step 800)
- Shows representation learning precedes generalization

**Bottom Right - Generalization Gap**:
- Orange shaded: Gap between train and test
- Gap closes during grokking
- Text box: Summary statistics

### Zoomed Figure (paper_02_grokking_zoomed.png)

- Focuses on steps 630-2030
- Clearly shows the smooth grokking transition
- Green shaded region: The 400-step grokking delay
- High detail view of how test "catches up" to train

---

## ğŸ“– Paper Context

**Title**: "Towards Understanding Grokking: An Effective Theory of Representation Learning"

**Main Contribution**: Provides theoretical framework for understanding grokking through:
1. Phase diagrams showing when grokking occurs
2. RQI metric to quantify representation quality
3. Connection to parallelogram structures in modular arithmetic
4. Effective theory explaining the dynamics

**Why our replication matters**: We confirmed the core grokking phenomenon using their exact experimental setup from Figure 4.

---

## âœ… Success Criteria Met

- [x] Training runs successfully
- [x] Proper hyperparameters from paper used
- [x] Grokking delay observed (400 steps)
- [x] Perfect final performance (100% train and test)
- [x] RQI metric extracted
- [x] Visualization created
- [x] Results match paper's claims

---

## ğŸš€ Impact on Project

**Updated Overall Progress:**

**âœ… Confirmed Grokking: 3/10 papers** â­â­â­
1. Paper 03 (Nanda et al.) - Sharp grokking on mod 113
2. Paper 05 (Liu et al. - Omnigrok) - Smooth grokking on MNIST
3. **Paper 02 (Liu et al. - Effective Theory)** - Grokking with RQI on mod 10

**Each paper demonstrates grokking differently:**
- **Different architectures**: Transformer vs MLP vs Encoder-Decoder
- **Different tasks**: Various modular arithmetic + vision
- **Different metrics**: Test acc jumps, RQI, progress measures
- **Different dynamics**: Sharp vs smooth, with/without weight decay

This diversity strengthens the evidence that grokking is a fundamental phenomenon!

---

## ğŸ“‚ Files Created

1. `run_paper02_proper.py` - Extraction script using train_add
2. `run_paper02_paper_params.sh` - SLURM submission script
3. `plot_paper02_grokking.py` - Visualization script
4. `logs/training_history.json` - Full training data
5. `analysis_results/paper_02_grokking.png` - Main visualization
6. `analysis_results/paper_02_grokking_zoomed.png` - Zoomed transition view
7. `PAPER02_RESULTS.md` - This summary document

---

**Bottom Line**: Paper 02 successfully replicated! Clear grokking with 400-step delay. RQI metric provides unique insight into representation learning dynamics.

**Next**: Continue systematic review of remaining papers to maximize grokking demonstrations.

